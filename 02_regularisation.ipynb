{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11c27fe",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helper functions\n",
    "from copy import deepcopy\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "import torch\n",
    "from capymoa.ann import LeNet5\n",
    "from capymoa.base import BatchClassifier\n",
    "from capymoa.classifier import Finetune\n",
    "from capymoa.ocl.base import TrainTaskAware\n",
    "from capymoa.ocl.datasets import SplitFashionMNIST\n",
    "from capymoa.ocl.evaluation import OCLMetrics, ocl_train_eval_loop\n",
    "from capymoa.ocl.strategy import ExperienceReplay\n",
    "from capymoa.stream import Schema\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from plot import plot_multiple, table\n",
    "\n",
    "stream = SplitFashionMNIST(normalize_features=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "schema = stream.schema\n",
    "results: Dict[str, OCLMetrics] = {}\n",
    "\n",
    "\n",
    "def run(learner: BatchClassifier) -> OCLMetrics:\n",
    "    torch.manual_seed(0)\n",
    "    return ocl_train_eval_loop(\n",
    "        learner,\n",
    "        stream.train_loaders(64),\n",
    "        stream.test_loaders(512),\n",
    "        continual_evaluations=1,\n",
    "        progress_bar=True,\n",
    "        eval_window_size=256,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed543b91",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "In this notebook, we implement L2, EWC, and SI based regularization for\n",
    "continual learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24110192",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "$$\n",
    "L_{reg} = \\lambda \\sum_{i} (\\theta_i - \\theta_i^*)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_reg(params: Iterable[Tensor], anchor_params: Iterable[Tensor]) -> Tensor:\n",
    "    l2 = 0.0\n",
    "    for param, anchor_param in zip(params, anchor_params, strict=True):\n",
    "        assert param.shape == anchor_param.shape\n",
    "        l2 += ((param - anchor_param) ** 2).sum()\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20736e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2RegularizedCL(BatchClassifier, TrainTaskAware):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: nn.Module,\n",
    "        lambda_: float,\n",
    "        lr: float = 0.01,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema)\n",
    "        self.lambda_ = lambda_\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.anchor_params: Optional[List[Tensor]] = None\n",
    "        self.test_task_id = 0\n",
    "\n",
    "    def batch_train(self, x: Tensor, y: Tensor) -> None:\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = self.model(x)\n",
    "        classify_loss = self.criterion(y_hat, y)\n",
    "        l2 = self.regularize()\n",
    "\n",
    "        # Backward pass and step\n",
    "        loss = classify_loss + self.lambda_ * l2\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def regularize(self) -> Tensor:\n",
    "        if self.anchor_params is not None:\n",
    "            return l2_reg(self.model.parameters(), self.anchor_params)\n",
    "        else:\n",
    "            return torch.scalar_tensor(0.0, device=self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def batch_predict_proba(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model.eval()(x)\n",
    "\n",
    "    def on_train_task(self, task_id: int):\n",
    "        # Adam maintains momentum, so we need to reinitialize the optimizer\n",
    "        # when the task changes\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # Anchoring the first task is detrimental\n",
    "        if task_id == 0:\n",
    "            self.anchor_params = None\n",
    "        else:\n",
    "            self.anchor_params = [\n",
    "                param.clone().detach().to(self.device)\n",
    "                for param in self.model.parameters()\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambda_ in [0, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    results[f\"L2 $\\\\lambda={lambda_:.1f}$\"] = run(\n",
    "        ExperienceReplay(\n",
    "            L2RegularizedCL(schema, LeNet5(10), lambda_=lambda_, lr=0.001), 100\n",
    "        )\n",
    "    )\n",
    "_ = plot_multiple(results, acc_seen=True, acc_online=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90732a78",
   "metadata": {},
   "source": [
    "From the figure we can conclude, that L2 regularization is:\n",
    "\n",
    "- slightly better than no regularization.\n",
    "- not very effective for continual learning.\n",
    "- can have a negative impact on online accuracy if the regularization\n",
    "  strength is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c18c3",
   "metadata": {},
   "source": [
    "## EWC\n",
    "\n",
    "$$\n",
    "L_{EWC} = \\frac{\\lambda}{2} \\sum_{i} F_i (\\theta_i - \\theta_i^*)^2\n",
    "$$\n",
    "where $F_i$ is the diagonal of the Fisher information matrix. The fisher\n",
    "information matrix measures how important a parameter is for the task.\n",
    "\n",
    "---\n",
    "1.  Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\n",
    "    Desjardins, Andrei A. Rusu, Kieran Milan, et al. **“Overcoming Catastrophic Forgetting\n",
    "    in Neural Networks.”** Proceedings of the National Academy of Sciences 114, no. 13\n",
    "    (March 28, 2017): 3521–26. https://doi.org/10.1073/pnas.1611835114."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717620f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l2_reg(\n",
    "    params: Iterable[Tensor],\n",
    "    anchor_params: Iterable[Tensor],\n",
    "    fisher_diags: Iterable[Tensor],\n",
    ") -> Tensor:\n",
    "    l2 = torch.scalar_tensor(0.0)\n",
    "    for param, anchor_param, fisher_diag in zip(\n",
    "        params, anchor_params, fisher_diags, strict=True\n",
    "    ):\n",
    "        assert param.shape == anchor_param.shape\n",
    "        l2 += (fisher_diag * (param - anchor_param) ** 2).sum()\n",
    "    return l2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capymoa.stream import Schema\n",
    "from torch import Tensor\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "\n",
    "class EWC(L2RegularizedCL):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: Module,\n",
    "        lambda_: float,\n",
    "        fim_buffer: int = 1000,\n",
    "        lr: float = 0.01,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema, model, lambda_, lr, device)\n",
    "        self._buffer_x: Tensor = torch.zeros(\n",
    "            (fim_buffer, schema.get_num_attributes()), device=device\n",
    "        )\n",
    "        self._buffer_y: Tensor = torch.zeros((fim_buffer,), device=device).long()\n",
    "        self._buffer_index = 0\n",
    "        self._buffer_size = fim_buffer\n",
    "        self.fim_diags: Optional[List[Tensor]] = None\n",
    "\n",
    "    def batch_train(self, x: Tensor, y: Tensor):\n",
    "        super().batch_train(x, y)\n",
    "        # Use the buffer as a ring buffer\n",
    "        for i in range(x.shape[0]):\n",
    "            self._buffer_x[self._buffer_index] = x[i]\n",
    "            self._buffer_y[self._buffer_index] = y[i]\n",
    "            self._buffer_index += 1\n",
    "            if self._buffer_index >= self._buffer_size:\n",
    "                self._buffer_index = 0\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def compute_fisher(self) -> List[Tensor]:\n",
    "        self.model.eval()\n",
    "        fisher_diags = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "        for x, y in zip(self._buffer_x, self._buffer_y):\n",
    "            x = x.unsqueeze(0)\n",
    "            y = y.unsqueeze(0)\n",
    "            self.model.zero_grad()\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "\n",
    "            for fisher_diag, param in zip(\n",
    "                fisher_diags, self.model.parameters(), strict=True\n",
    "            ):\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                fisher_diag.add_(param.grad.detach() ** 2)\n",
    "\n",
    "        for fisher_diag in fisher_diags:\n",
    "            fisher_diag.div_(self._buffer_size)\n",
    "        return fisher_diags\n",
    "\n",
    "    def on_train_task(self, task_id: int):\n",
    "        # Adam maintains momentum, so we need to reinitialize the optimizer\n",
    "        # when the task changes\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        if task_id == 0:\n",
    "            return\n",
    "        print(f\"Computing Fisher information matrix for task {task_id}\")\n",
    "        fim_diags = self.compute_fisher()\n",
    "        if self.fim_diags is None:\n",
    "            self.fim_diags = fim_diags\n",
    "        else:\n",
    "            for i in range(len(self.fim_diags)):\n",
    "                self.fim_diags[i] += fim_diags[i]\n",
    "\n",
    "    def regularize(self) -> Tensor:\n",
    "        if self.anchor_params is not None and self.fim_diags is not None:\n",
    "            return weighted_l2_reg(\n",
    "                self.model.parameters(), self.anchor_params, self.fim_diags\n",
    "            )\n",
    "        else:\n",
    "            return torch.scalar_tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "results.clear()\n",
    "for lambda_ in [\n",
    "    0,\n",
    "    # 0.1,\n",
    "    5,\n",
    "]:\n",
    "    results[f\"EWC $\\\\lambda={lambda_:.1f}$\"] = run(\n",
    "        EWC(schema, LeNet5(10), lambda_=lambda_, lr=0.01)\n",
    "    )\n",
    "_ = plot_multiple(results, acc_seen=True, acc_online=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bc1fd",
   "metadata": {},
   "source": [
    "## Learning without Forgetting (LWF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77dbdee",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "Knowledge distillation loss function from Hinton et al. (2015) [1][3].\n",
    "\n",
    "A type of response-based distillation that forces the student to mimic the output class probabilities of the teacher model. The loss is calculated as:\n",
    "\n",
    "$$\n",
    "L_{KD} = t^2 \\times KL\\left(\n",
    "    \\text{softmax}(\\mathbf{a}/t),\n",
    "    \\text{softmax}(\\mathbf{b}/t)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $t$ is the temperature for the softmax function.\n",
    "- $\\mathbf{a}$ is the output (logits) from the teacher model.\n",
    "- $\\mathbf{b}$ is the output (logits) from the student model.\n",
    "- $\\mathbf{x}$ is the input to the model.\n",
    "\n",
    "The original paper uses the cross-entropy loss between the soft targets and soft predictions. Cross-entropy loss can be defined as the Kullback-Leibler (KL) divergence plus the entropy of the target distribution [2]. The entropy of the target distribution is irrelevant to optimization since it is a constant. It is removed in this implementation, which is nice since the loss will equal zero when the student model matches the teacher model.\n",
    "\n",
    "---\n",
    "[1] Geoffrey Hinton, Oriol Vinyals, Jeff Dean (2015) Distilling the Knowledge in a Neural Network  \n",
    "[2] https://en.wikipedia.org/wiki/Cross-entropy  \n",
    "[3] https://intellabs.github.io/distiller/knowledge_distillation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa24b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import kl_div, log_softmax\n",
    "\n",
    "\n",
    "def hinton_kdistill_loss(\n",
    "    teacher_logits: Tensor, student_logits: Tensor, temperature: float\n",
    ") -> Tensor:\n",
    "    return (\n",
    "        kl_div(\n",
    "            log_softmax(student_logits / temperature, dim=1),  # Soft predictions\n",
    "            log_softmax(teacher_logits / temperature, dim=1),  # Soft targets\n",
    "            log_target=True,\n",
    "            reduction=\"batchmean\",  # Mathematically correct unlike the default\n",
    "        )\n",
    "        * temperature**2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470613d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWF(BatchClassifier, TrainTaskAware):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: nn.Module,\n",
    "        lambda_: float,\n",
    "        lr: float = 0.001,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema)\n",
    "        self.lambda_ = lambda_\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.model = model.to(self.device)\n",
    "        self.teacher: Optional[nn.Module] = None\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def batch_train(self, x: Tensor, y: Tensor):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        ce_loss = self.criterion(y_hat, y)\n",
    "        kd_loss = self.kd_loss(x, y_hat)\n",
    "        loss = ce_loss + self.lambda_ * kd_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def kd_loss(self, x: Tensor, student_logits: Tensor) -> Tensor:\n",
    "        if self.teacher is None:\n",
    "            return torch.scalar_tensor(0.0, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(x)\n",
    "        return hinton_kdistill_loss(\n",
    "            teacher_logits=teacher_logits,\n",
    "            student_logits=student_logits,\n",
    "            temperature=2.0,\n",
    "        )\n",
    "\n",
    "    def on_train_task(self, task_id: int):\n",
    "        if task_id == 0:\n",
    "            return\n",
    "        print(f\"Updating Teacher: {task_id}\")\n",
    "        self.teacher = deepcopy(self.model)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def batch_predict_proba(self, x: Tensor) -> Tensor:\n",
    "        return self.model.eval()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.clear()\n",
    "results[\"LWF $\\\\lambda=0.5$\"] = run(\n",
    "    ExperienceReplay(LWF(schema, LeNet5(10), lambda_=0.5), 200, repeat=2)\n",
    ")\n",
    "results[\"ER\"] = run(\n",
    "    ExperienceReplay(Finetune(schema, LeNet5(10), device=device), 200, repeat=2)\n",
    ")\n",
    "_ = plot_multiple(results, acc_seen=True, acc_online=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capymoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
