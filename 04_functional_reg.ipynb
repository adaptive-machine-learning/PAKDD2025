{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from capymoa.ann import Perceptron\n",
    "from capymoa.base import BatchClassifier\n",
    "from capymoa.classifier import Finetune\n",
    "from capymoa.instance import Instance\n",
    "from capymoa.ocl.ann import WNPerceptron\n",
    "from capymoa.ocl.base import TaskBoundaryAware\n",
    "from capymoa.ocl.datasets import SplitMNIST\n",
    "from capymoa.ocl.evaluation import ocl_train_eval_loop\n",
    "from capymoa.ocl.strategy import ExperienceReplay\n",
    "from capymoa.stream import Schema\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor, nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Noto Sans\"]\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "pt = 1 / 72.27\n",
    "figsize_169 = (455 * pt, 256 * pt)\n",
    "figsize = (figsize_169[0], 0.45 * figsize_169[0])\n",
    "scenario = SplitMNIST()\n",
    "schema = scenario.schema\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import kl_div, log_softmax\n",
    "\n",
    "\n",
    "def hinton_kdistill_loss(\n",
    "    teacher_logits: Tensor, student_logits: Tensor, temperature: float\n",
    ") -> Tensor:\n",
    "    r\"\"\"Knowledge distillation loss function from Hinton et al. (2015) [1]_ [3]_.\n",
    "\n",
    "    A type of response based distillation that forces the student to mimic the\n",
    "    output class probabilities of the teacher model. The loss is calculated as:\n",
    "\n",
    "    .. math::\n",
    "        L_{KD} =  t^2 \\times KL(\n",
    "            \\text{softmax}(\\mathbf{a}{t}),\n",
    "            \\text{softmax}(\\mathbf{b}{t}))\n",
    "\n",
    "    where:\n",
    "\n",
    "    * :math:`t` is the temperature for the softmax function.\n",
    "    * :math:`\\mathbf{a}` is the output (logits) from the teacher model.\n",
    "    * :math:`\\mathbf{b}` is the output (logits) from the student model.\n",
    "    * :math:`\\mathbf{x}` is the input to the model.\n",
    "\n",
    "    The original paper uses the cross-entropy loss between the soft targets and\n",
    "    soft predictions. Cross entropy loss can be defined as the Kullback-Leibler\n",
    "    (KL) divergence plus the entropy of the target distribution [2]_. The entropy\n",
    "    of the target distribution is irrelevant to optimisation since it is a\n",
    "    constant. It is removed in this implementation, which is nice since the loss\n",
    "    will equal zero when the student model matches the teacher model.\n",
    "\n",
    "\n",
    "    .. [1] Geoffrey Hinton, Oriol Vinyals, Jeff Dean  (2015) Distilling the\n",
    "        Knowledge in a Neural Network\n",
    "\n",
    "    .. [2] https://en.wikipedia.org/wiki/Cross-entropy\n",
    "\n",
    "    .. [3] https://intellabs.github.io/distiller/knowledge_distillation.html\n",
    "\n",
    "    :param teacher_logits: The output from the teacher model in shape\n",
    "        ``(batch_size, n_classes)``\n",
    "    :param student_logits: The output from the student model in shape\n",
    "        ``(batch_size, n_classes)``\n",
    "    :param temperature: The temperature for the softmax function\n",
    "    :return: The knowledge distillation loss.\n",
    "    \"\"\"\n",
    "    if teacher_logits.ndim != 2 or student_logits.ndim != 2:\n",
    "        raise ValueError(\n",
    "            \"Teacher and student logits must have two dimensions, \"\n",
    "            f\" but got {teacher_logits.ndim} and {student_logits.ndim}\"\n",
    "        )\n",
    "    if teacher_logits.shape != student_logits.shape:\n",
    "        raise ValueError(\n",
    "            \"Teacher and student logits must have the same shape, \"\n",
    "            f\" but got {teacher_logits.shape} and {student_logits.shape}\"\n",
    "        )\n",
    "    # Calculate the soft targets and soft predictions\n",
    "    return (\n",
    "        kl_div(\n",
    "            log_softmax(student_logits / temperature, dim=1),  # Soft predictions\n",
    "            log_softmax(teacher_logits / temperature, dim=1),  # Soft targets\n",
    "            log_target=True,\n",
    "            reduction=\"batchmean\",  # Mathematically correct unlike the default\n",
    "        )\n",
    "        * temperature**2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWF(BatchClassifier, TaskBoundaryAware):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: nn.Module,\n",
    "        lambda_: float,\n",
    "        batch_size: int = 128,\n",
    "        random_seed: int = 1,\n",
    "        lr: float = 0.01,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema, batch_size, random_seed)\n",
    "        self.lambda_ = lambda_\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.model = model.to(self.device)\n",
    "        self.teacher: Optional[nn.Module] = None\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def batch_train(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.torch_batch_train(\n",
    "            torch.from_numpy(x).float().to(self.device),\n",
    "            torch.from_numpy(y).long().to(self.device),\n",
    "        )\n",
    "\n",
    "    def torch_batch_train(self, x: Tensor, y: Tensor):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        ce_loss = self.criterion(y_hat, y)\n",
    "        kd_loss = self.kd_loss(x, y_hat)\n",
    "        loss = ce_loss + self.lambda_ * kd_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def kd_loss(self, x: Tensor, student_logits: Tensor) -> Tensor:\n",
    "        if self.teacher is None:\n",
    "            return torch.scalar_tensor(0.0, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(x)\n",
    "        return hinton_kdistill_loss(\n",
    "            teacher_logits=teacher_logits,\n",
    "            student_logits=student_logits,\n",
    "            temperature=2.0,\n",
    "        )\n",
    "\n",
    "    def set_train_task(self, train_task_id: int):\n",
    "        # Adam maintains momentum, so we need to reinitialize the optimizer\n",
    "        # when the task changes\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        if train_task_id == 0:\n",
    "            return\n",
    "        self.teacher = deepcopy(self.model)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, instance: Instance) -> np.ndarray:\n",
    "        x = torch.from_numpy(instance.x).float().to(self.device).view(1, -1)\n",
    "        self.model.eval()\n",
    "\n",
    "        y_hat = self.model(x)\n",
    "        return torch.softmax(y_hat, dim=1).cpu().numpy()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"LWF(lambda_={self.lambda_}, model={self.model})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "lwf = LWF(schema, Perceptron(schema, 128), lambda_=0.5, lr=1e-3)\n",
    "learner = ExperienceReplay(lwf, 100)\n",
    "lwf_results = ocl_train_eval_loop(\n",
    "    learner, scenario.train_streams, scenario.test_streams, progress_bar=True\n",
    ")\n",
    "\n",
    "er = ExperienceReplay(\n",
    "    Finetune(schema, WNPerceptron(schema, 128), 100, device=device), 100\n",
    ")\n",
    "er_results = ocl_train_eval_loop(\n",
    "    er, scenario.train_streams, scenario.test_streams, progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lambda_ in [0.1, 0.5, 1.0]:\n",
    "#     torch.manual_seed(0)\n",
    "#     lwf = LWF(schema, Percep(schema, 128), lambda_=lambda_, lr=1e-3)\n",
    "#     learner = ExperienceReplay(lwf, 100)\n",
    "#     lwf_results = ocl_train_eval_loop(learner, scenario.train_streams, scenario.test_streams, progress_bar=False)\n",
    "#     print(lambda_, lwf_results.accuracy_all_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from plot import plot_multiple\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize, layout=\"constrained\")\n",
    "plot_multiple(\n",
    "    [\n",
    "        (r\"LWF n=100\", lwf_results),\n",
    "        (r\"ER n=100\", er_results),\n",
    "    ],\n",
    "    ax,\n",
    "    # acc_all=True,\n",
    "    acc_seen=True,\n",
    "    acc_online=True,\n",
    ")\n",
    "ax.set_title(\"SplitMNIST10/5\")\n",
    "plt.savefig(\"fig/lwf.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capymoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
