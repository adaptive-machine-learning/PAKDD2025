{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed543b91",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "In this notebook, we implement L2, EWC, and SI based regularization for\n",
    "continual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from capymoa.ocl.ann import WNPerceptron\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import Tensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Noto Sans\"]\n",
    "plt.rcParams[\"font.size\"] = 9\n",
    "pt = 1 / 72.27\n",
    "figsize_169 = (455 * pt, 256 * pt)\n",
    "figsize = (figsize_169[0], 0.45 * figsize_169[0])\n",
    "cl_evals = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24110192",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{reg} = \\lambda \\sum_{i} (\\theta_i - \\theta_i^*)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_reg(params: Iterable[Tensor], anchor_params: Iterable[Tensor]) -> Tensor:\n",
    "    l2 = torch.scalar_tensor(0.0)\n",
    "    for param, anchor_param in zip(params, anchor_params, strict=True):\n",
    "        assert param.shape == anchor_param.shape\n",
    "        l2 += ((param - anchor_param) ** 2).sum()\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20736e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from capymoa.base import BatchClassifier\n",
    "from capymoa.instance import Instance\n",
    "from capymoa.ocl.base import TaskBoundaryAware\n",
    "from capymoa.stream._stream import Schema\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class L2RegularizedCL(BatchClassifier, TaskBoundaryAware):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: nn.Module,\n",
    "        lambda_: float,\n",
    "        batch_size: int = 128,\n",
    "        random_seed: int = 1,\n",
    "        lr: float = 0.01,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema, batch_size, random_seed)\n",
    "        self.lambda_ = lambda_\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.anchor_params: Optional[List[Tensor]] = None\n",
    "        self.test_task_id = 0\n",
    "\n",
    "    def batch_train(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.torch_batch_train(\n",
    "            torch.from_numpy(x).float().to(self.device),\n",
    "            torch.from_numpy(y).long().to(self.device),\n",
    "        )\n",
    "\n",
    "    def torch_batch_train(self, x: Tensor, y: Tensor):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = self.model(x)\n",
    "        classify_loss = self.criterion(y_hat, y)\n",
    "        l2 = self.regularize()\n",
    "        # print(f\"l2: {l2.item()} classify_loss: {classify_loss.item()}\")\n",
    "\n",
    "        # Backward pass and step\n",
    "        loss = classify_loss + self.lambda_ * l2\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def regularize(self) -> Tensor:\n",
    "        if self.anchor_params is not None:\n",
    "            return l2_reg(self.model.parameters(), self.anchor_params)\n",
    "        else:\n",
    "            return torch.scalar_tensor(0.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, instance: Instance) -> np.ndarray:\n",
    "        x = torch.from_numpy(instance.x).float().to(self.device).view(1, -1)\n",
    "        self.model.eval()\n",
    "\n",
    "        y_hat = self.model(x)\n",
    "        return torch.softmax(y_hat, dim=1).cpu().numpy()\n",
    "\n",
    "    def set_train_task(self, train_task_id: int):\n",
    "        # Adam maintains momentum, so we need to reinitialize the optimizer\n",
    "        # when the task changes\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # Anchoring the first task is detrimental\n",
    "        if train_task_id == 0:\n",
    "            self.anchor_params = None\n",
    "        else:\n",
    "            self.anchor_params = [\n",
    "                param.clone().detach() for param in self.model.parameters()\n",
    "            ]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"L2RegularizedCL(lambda_={self.lambda_}, lr={self.optimizer.defaults['lr']})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capymoa.ocl.datasets import SplitMNIST\n",
    "from capymoa.ocl.evaluation import ocl_train_eval_loop\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "scenario = SplitMNIST()\n",
    "schema = scenario.schema\n",
    "l2_off = L2RegularizedCL(schema, WNPerceptron(schema, 128), lambda_=0)\n",
    "l2_low = L2RegularizedCL(schema, WNPerceptron(schema, 128), lambda_=1)\n",
    "l2_high = L2RegularizedCL(schema, WNPerceptron(schema, 128), lambda_=5)\n",
    "\n",
    "l2_off_results = ocl_train_eval_loop(\n",
    "    l2_off,\n",
    "    scenario.train_streams,\n",
    "    scenario.test_streams,\n",
    "    progress_bar=True,\n",
    "    continual_evaluations=cl_evals,\n",
    ")\n",
    "l2_low_results = ocl_train_eval_loop(\n",
    "    l2_low,\n",
    "    scenario.train_streams,\n",
    "    scenario.test_streams,\n",
    "    progress_bar=True,\n",
    "    continual_evaluations=cl_evals,\n",
    ")\n",
    "l2_high_results = ocl_train_eval_loop(\n",
    "    l2_high,\n",
    "    scenario.train_streams,\n",
    "    scenario.test_streams,\n",
    "    progress_bar=True,\n",
    "    continual_evaluations=cl_evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b27f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from plot import plot_multiple\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_multiple(\n",
    "    [\n",
    "        (r\"Control $\\lambda=0$\", l2_off_results),\n",
    "        (r\"L2RegularizedCL $\\lambda=1$\", l2_low_results),\n",
    "        (r\"L2RegularizedCL $\\lambda=5$\", l2_high_results),\n",
    "    ],\n",
    "    ax,\n",
    "    # acc_all=True,\n",
    "    acc_seen=True,\n",
    "    acc_online=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90732a78",
   "metadata": {},
   "source": [
    "From the figure we can conclude, that L2 regularization is:\n",
    "\n",
    "- slightly better than no regularization.\n",
    "- not very effective for continual learning.\n",
    "- can have a negative impact on online accuracy if the regularization\n",
    "  strength is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c18c3",
   "metadata": {},
   "source": [
    "# EWC\n",
    "\n",
    "$$\n",
    "L_{EWC} = \\frac{\\lambda}{2} \\sum_{i} F_i (\\theta_i - \\theta_i^*)^2\n",
    "$$\n",
    "where $F_i$ is the diagonal of the Fisher information matrix. The fisher\n",
    "information matrix measures how important a parameter is for the task.\n",
    "\n",
    "* Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. “Overcoming Catastrophic Forgetting in Neural Networks.” Proceedings of the National Academy of Sciences 114, no. 13 (March 28, 2017): 3521–26. https://doi.org/10.1073/pnas.1611835114.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717620f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_l2_reg(\n",
    "    params: Iterable[Tensor],\n",
    "    anchor_params: Iterable[Tensor],\n",
    "    fisher_diags: Iterable[Tensor],\n",
    ") -> Tensor:\n",
    "    l2 = torch.scalar_tensor(0.0)\n",
    "    for param, anchor_param, fisher_diag in zip(\n",
    "        params, anchor_params, fisher_diags, strict=True\n",
    "    ):\n",
    "        assert param.shape == anchor_param.shape\n",
    "        l2 += (fisher_diag * (param - anchor_param) ** 2).sum()\n",
    "    return l2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5950a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capymoa.stream import Schema\n",
    "from torch import Tensor\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "\n",
    "class EWC(L2RegularizedCL):\n",
    "    def __init__(\n",
    "        self,\n",
    "        schema: Schema,\n",
    "        model: Module,\n",
    "        lambda_: float,\n",
    "        fim_buffer: int = 1000,\n",
    "        batch_size: int = 128,\n",
    "        random_seed: int = 1,\n",
    "        lr: float = 0.01,\n",
    "        device: torch.device = device,\n",
    "    ) -> None:\n",
    "        super().__init__(schema, model, lambda_, batch_size, random_seed, lr, device)\n",
    "        self._buffer_x: Tensor = torch.zeros(\n",
    "            (fim_buffer, schema.get_num_attributes()), device=device\n",
    "        )\n",
    "        self._buffer_y: Tensor = torch.zeros((fim_buffer,), device=device).long()\n",
    "        self._buffer_index = 0\n",
    "        self._buffer_size = fim_buffer\n",
    "        self.fim_diags: Optional[List[Tensor]] = None\n",
    "\n",
    "    def torch_batch_train(self, x: Tensor, y: Tensor):\n",
    "        super().torch_batch_train(x, y)\n",
    "        # Use the buffer as a ring buffer\n",
    "        for i in range(x.shape[0]):\n",
    "            self._buffer_x[self._buffer_index] = x[i]\n",
    "            self._buffer_y[self._buffer_index] = y[i]\n",
    "            self._buffer_index += 1\n",
    "            if self._buffer_index >= self._buffer_size:\n",
    "                self._buffer_index = 0\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def compute_fisher(self) -> List[Tensor]:\n",
    "        self.model.eval()\n",
    "        # dataset = TensorDataset(self._buffer_x, self._buffer_y)\n",
    "        # dataloader = torch.utils.data.DataLoader(\n",
    "        #     dataset, batch_size=self.batch_size, shuffle=False\n",
    "        # )\n",
    "        # fim_diag = FIM(\n",
    "        #     self.model,\n",
    "        #     dataloader,\n",
    "        #     representation=PMatDiag,\n",
    "        # )\n",
    "        # fisher_diagonals = [\n",
    "        #     torch.zeros_like(param) for param in self.model.parameters()\n",
    "        # ]\n",
    "        # vector_to_parameters(fim_diag.get_diag(), fisher_diagonals)\n",
    "        # return fisher_diagonals\n",
    "\n",
    "        fisher_diags = [torch.zeros_like(param) for param in self.model.parameters()]\n",
    "\n",
    "        for x, y in zip(self._buffer_x, self._buffer_y):\n",
    "            x = x.unsqueeze(0)\n",
    "            y = y.unsqueeze(0)\n",
    "            self.model.zero_grad()\n",
    "            y_hat = self.model(x)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "\n",
    "            for fisher_diag, param in zip(\n",
    "                fisher_diags, self.model.parameters(), strict=True\n",
    "            ):\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                fisher_diag.add_(param.grad.detach() ** 2)\n",
    "\n",
    "        for fisher_diag in fisher_diags:\n",
    "            fisher_diag.div_(self._buffer_size)\n",
    "        return fisher_diags\n",
    "\n",
    "    def set_train_task(self, train_task_id: int):\n",
    "        super().set_train_task(train_task_id)\n",
    "        if train_task_id == 0:\n",
    "            return\n",
    "        super().set_train_task(train_task_id)\n",
    "        print(f\"Computing Fisher information matrix for task {train_task_id}\")\n",
    "        fim_diags = self.compute_fisher()\n",
    "        if self.fim_diags is None:\n",
    "            self.fim_diags = fim_diags\n",
    "        else:\n",
    "            for i in range(len(self.fim_diags)):\n",
    "                self.fim_diags[i] += fim_diags[i]\n",
    "\n",
    "    def regularize(self) -> Tensor:\n",
    "        if self.anchor_params is not None and self.fim_diags is not None:\n",
    "            return weighted_l2_reg(\n",
    "                self.model.parameters(), self.anchor_params, self.fim_diags\n",
    "            )\n",
    "        else:\n",
    "            return torch.scalar_tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "ewc = EWC(schema, WNPerceptron(schema, 128), lambda_=10_000, lr=0.001)\n",
    "ewc_results = ocl_train_eval_loop(\n",
    "    ewc,\n",
    "    scenario.train_streams,\n",
    "    scenario.test_streams,\n",
    "    progress_bar=True,\n",
    "    continual_evaluations=cl_evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from plot import plot_multiple\n",
    "\n",
    "fig, ax = plt.subplots(figsize=figsize, layout=\"constrained\")\n",
    "plot_multiple(\n",
    "    [\n",
    "        (r\"Control $\\lambda=0$\", l2_off_results),\n",
    "        (r\"L2 $\\lambda=1$\", l2_low_results),\n",
    "        (r\"L2 $\\lambda=5$\", l2_high_results),\n",
    "        (r\"EWC $\\lambda=1$\", ewc_results),\n",
    "    ],\n",
    "    ax,\n",
    "    # acc_all=True,\n",
    "    acc_seen=True,\n",
    "    acc_online=True,\n",
    ")\n",
    "ax.set_title(\"SplitMNIST10/5\")\n",
    "plt.savefig(\"fig/regularization.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capymoa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
